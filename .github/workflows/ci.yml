name: CI/CD Pipeline v2.0

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main ]
  release:
    types: [ published ]

jobs:
  test:
    runs-on: ${{ matrix.os }}
    strategy:
      matrix:
        os: [ubuntu-latest, windows-latest, macos-latest]
        python-version: ['3.8', '3.9', '3.10', '3.11', '3.12']
        exclude:
          # macOS M1 runners have limited Python version support
          - os: macos-latest
            python-version: '3.8'

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v5
      with:
        python-version: ${{ matrix.python-version }}

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov

    - name: Create test directories
      run: python -c "import os; os.makedirs('test_input', exist_ok=True); os.makedirs('test_output', exist_ok=True); print('Directories created')"

    - name: Create sample test data
      run: |
        python -c "
        import pandas as pd
        import os
        
        # Create sample CSV data for testing
        sample_data = {
            'profile_name': ['Airport A'] * 50 + ['Airport B'] * 50,
            'score': [1, 2, 3, 4, 5] * 20,
            'comment': ['Test comment ' + str(i) for i in range(100)],
            'language': ['en'] * 40 + ['tr'] * 30 + ['de'] * 30,
            'contribution': ['Test contribution'] * 100,
            'date': ['2024-01-01'] * 100,
            'url': ['https://test.com'] * 100
        }
        
        df = pd.DataFrame(sample_data)
        df.to_csv('test_input/test_data.csv', index=False)
        print('Sample test data created successfully')
        print(f'Created file with {len(df)} rows')
        "

    - name: Test basic functionality
      run: |
        python -c "
        import sys
        import os
        
        # Verify directories exist
        assert os.path.exists('test_input'), 'test_input directory not found'
        assert os.path.exists('test_output'), 'test_output directory not found'
        assert os.path.exists('test_input/test_data.csv'), 'test data file not found'
        
        # Test basic functionality
        from smart_mix_split import SmartMixSplit
        
        mixer = SmartMixSplit('test_input', 'test_output')
        data = mixer.read_csv_files()
        print('SUCCESS: Data loaded successfully - Shape:', data.shape)
        
        analysis = mixer.analyze_data()
        print('SUCCESS: Analysis completed - Rows:', analysis['total_rows'])
        
        # Test sampling
        sample = mixer.create_balanced_sample({5: 5, 4: 5, 3: 5, 2: 3, 1: 2})
        print('SUCCESS: Sampling completed - Shape:', sample.shape)
        
        mixer.save_results(sample, 'test_sample.csv')
        
        # Verify output file was created
        assert os.path.exists('test_output/test_sample.csv'), 'Output file not created'
        print('SUCCESS: All basic tests passed!')
        "

    - name: Test example usage (non-interactive)
      run: |
        python -c "
        import os
        from smart_mix_split import SmartMixSplit
        
        print('Testing example usage scenarios...')
        
        mixer = SmartMixSplit('test_input', 'test_output')
        data = mixer.read_csv_files()
        analysis = mixer.analyze_data()
        
        # Test predefined distributions
        test_distributions = [
            {'name': 'balanced', 'dist': {5: 5, 4: 5, 3: 5, 2: 3, 1: 2}},
            {'name': 'positive_focus', 'dist': {5: 8, 4: 6, 3: 4, 2: 1, 1: 1}},
            {'name': 'equal_weight', 'dist': {5: 3, 4: 3, 3: 3, 2: 3, 1: 3}}
        ]
        
        for i, test_case in enumerate(test_distributions, 1):
            try:
                sample = mixer.create_balanced_sample(test_case['dist'])
                filename = f'test_example_{i}_{test_case[\"name\"]}.csv'
                mixer.save_results(sample, filename)
                
                # Verify file was created
                output_path = os.path.join('test_output', filename)
                assert os.path.exists(output_path), f'Output file {filename} not created'
                
                print(f'SUCCESS: Test case {i} ({test_case[\"name\"]}) completed successfully')
            except Exception as e:
                print(f'ERROR: Test case {i} failed: {e}')
                raise
        
        print('SUCCESS: All example tests passed!')
        "

    - name: Lint with flake8 (optional)
      run: |
        pip install flake8
        # Stop the build if there are Python syntax errors or undefined names
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        # Exit-zero treats all errors as warnings
        flake8 . --count --exit-zero --max-complexity=10 --max-line-length=127 --statistics
      continue-on-error: true

  package:
    needs: test
    runs-on: ubuntu-latest
    if: github.event_name == 'release'

    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install packaging tools
      run: |
        python -m pip install --upgrade pip
        pip install build twine

    - name: Build package
      run: |
        python -m build

    - name: Check package
      run: |
        twine check dist/*

    - name: Upload artifacts
      uses: actions/upload-artifact@v3
      with:
        name: dist
        path: dist/

  security:
    runs-on: ubuntu-latest
    steps:
    - uses: actions/checkout@v4

    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install bandit safety

    - name: Run security checks with bandit
      run: |
        bandit -r . -f json -o bandit-report.json
      continue-on-error: true

    - name: Check dependencies with safety
      run: |
        safety check --json --output safety-report.json
      continue-on-error: true

    - name: Upload security reports
      uses: actions/upload-artifact@v3
      with:
        name: security-reports
        path: |
          bandit-report.json
          safety-report.json
      if: always()
